{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(verbose=True)\n",
    "key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader1 = TextLoader(\"./data/nlp-keywords.txt\", encoding='utf-8')\n",
    "loader2 = TextLoader(\"./data/book_document.txt\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서 분할\n",
    "split_doc1 = loader1.load_and_split(text_splitter)\n",
    "split_doc2 = loader2.load_and_split(text_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536\n"
     ]
    }
   ],
   "source": [
    "# 임베딩 차원 크기를 계산\n",
    "dimension_size = len(embeddings.embed_query(\"hello world\"))\n",
    "print(dimension_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **FAISS 벡터 저장소 생성 (from_documents)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`from_documents()` 문서 리스트와 임베딩 함수를 사용하여 FAISS 벡터 저장소를 생성합니다.\n",
    "\n",
    "**매개변수**\n",
    "<br>\n",
    "\n",
    "- documents (List[Document])  : 벡터 저장소에 추가할 문서 리스트\n",
    "- embedding (Embeddings)      : 사용할 임베딩 함수\n",
    "- **kwargs                    : 추가 키워드 인자\n",
    "\n",
    "동작\n",
    "<br>\n",
    "\n",
    "1. 문서 리스트에서 텍스트 내용(page_content)과 메타데이터를 추출합니다. <br>\n",
    "2. 추출한 텍스트와 메타데이터를 사용하여 from_texts 메서드를 호출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DB 생성\n",
    "db = FAISS.from_documents(\n",
    "    documents=split_doc1+split_doc2, \n",
    "    embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '396891ae-0fc9-4537-b096-2dd5513845e9',\n",
       " 1: '48b7f169-e9ed-4d93-af52-c147d412db7e',\n",
       " 2: 'a95a3e60-e225-4417-970f-d13052664ef2',\n",
       " 3: '64d6efc8-5a12-4b36-be20-c649795800b8',\n",
       " 4: '42ac4795-9eb6-4a01-8855-993dbd613244',\n",
       " 5: '19bf47a9-28d2-44ff-be98-c4ff7a3137ec',\n",
       " 6: '8f034c61-fa98-40e7-8da4-b4c56c368a28',\n",
       " 7: '5035fe87-8daa-4430-9823-4e49eb2cd860',\n",
       " 8: '0ef161b0-1fbf-4b46-a883-10c4d97ca5a3',\n",
       " 9: 'a1d79ca6-26fa-4e0a-bdb9-82645ac44739',\n",
       " 10: 'b8fbb070-ff31-4bb0-9393-2078ca8c75ce',\n",
       " 11: 'd1e952de-602d-47ec-b45a-371343298ec3',\n",
       " 12: '79c805a1-e629-496b-bc0c-d88386a79f90',\n",
       " 13: '0e4861b0-8b20-4ca1-82ff-68ecb1edc177',\n",
       " 14: 'e4d1dc56-9095-4f9a-a01a-64376218fd45'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문서 저장소 ID 확인\n",
    "db.index_to_docstore_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 문서의 ID: Document 확인\n",
    "# db.docstore._dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 유사도 검색 (Similarity Search)\n",
    "\n",
    "**매개변수**\n",
    "<br>\n",
    "- query (str): 유사한 문서를 찾기 위한 검색 쿼리 텍스트\n",
    "- k (int): 반환할 문서 수. 기본값은 4\n",
    "- filter (Optional[Union[Callable, Dict[str, Any]]]): 메타데이터 필터링 함수 또는 딕셔너리. 기본값은 None\n",
    "- fetch_k (int): 필터링 전에 가져올 문서 수. 기본값은 20\n",
    "- **kwargs: 추가 키워드 인자\n",
    "\n",
    "동작\n",
    "<br>\n",
    "\n",
    "1. similarity_search_with_score()를 내부적으로 호출하여 유사도 점수와 함께 문서를 검색합니다.\n",
    "2. 검색 결과에서 점수를 제외하고 문서만 추출하여 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './data/nlp-keywords.txt'}, page_content='정의: TF-IDF는 문서 내에서 단어의 중요도를 평가하는 데 사용되는 통계적 척도입니다. 이는 문서 내 단어의 빈도와 전체 문서 집합에서 그 단어의 희소성을 고려합니다.\\n예시: 많은 문서에서 자주 등장하지 않는 단어는 높은 TF-IDF 값을 가집니다.\\n연관키워드: 자연어 처리, 정보 검색, 데이터 마이닝\\n\\nDeep Learning\\n\\n정의: 딥러닝은 인공신경망을 이용하여 복잡한 문제를 해결하는 머신러닝의 한 분야입니다. 이는 데이터에서 고수준의 표현을 학습하는 데 중점을 둡니다.\\n예시: 이미지 인식, 음성 인식, 자연어 처리 등에서 딥러닝 모델이 활용됩니다.\\n연관키워드: 인공신경망, 머신러닝, 데이터 분석\\n\\nSchema\\n\\n정의: 스키마는 데이터베이스나 파일의 구조를 정의하는 것으로, 데이터가 어떻게 저장되고 조직되는지에 대한 청사진을 제공합니다.\\n예시: 관계형 데이터베이스의 테이블 스키마는 열 이름, 데이터 타입, 키 제약 조건 등을 정의합니다.\\n연관키워드: 데이터베이스, 데이터 모델링, 데이터 관리\\n\\nDataFrame'),\n",
       " Document(metadata={'source': './data/nlp-keywords.txt'}, page_content='정의: 오픈 소스는 소스 코드가 공개되어 누구나 자유롭게 사용, 수정, 배포할 수 있는 소프트웨어를 의미합니다. 이는 협업과 혁신을 촉진하는 데 중요한 역할을 합니다.\\n예시: 리눅스 운영 체제는 대표적인 오픈 소스 프로젝트입니다.\\n연관키워드: 소프트웨어 개발, 커뮤니티, 기술 협업\\n\\nStructured Data\\n\\n정의: 구조화된 데이터는 정해진 형식이나 스키마에 따라 조직된 데이터입니다. 이는 데이터베이스, 스프레드시트 등에서 쉽게 검색하고 분석할 수 있습니다.\\n예시: 관계형 데이터베이스에 저장된 고객 정보 테이블은 구조화된 데이터의 예입니다.\\n연관키워드: 데이터베이스, 데이터 분석, 데이터 모델링\\n\\nParser\\n\\n정의: 파서는 주어진 데이터(문자열, 파일 등)를 분석하여 구조화된 형태로 변환하는 도구입니다. 이는 프로그래밍 언어의 구문 분석이나 파일 데이터 처리에 사용됩니다.\\n예시: HTML 문서를 구문 분석하여 웹 페이지의 DOM 구조를 생성하는 것은 파싱의 한 예입니다.\\n연관키워드: 구문 분석, 컴파일러, 데이터 처리\\n\\nTF-IDF (Term Frequency-Inverse Document Frequency)'),\n",
       " Document(metadata={'source': './data/nlp-keywords.txt'}, page_content='정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\\n예시: Word2Vec 모델에서 \"왕\"과 \"여왕\"은 서로 가까운 위치에 벡터로 표현됩니다.\\n연관키워드: 자연어 처리, 임베딩, 의미론적 유사성\\nLLM (Large Language Model)\\n\\n정의: LLM은 대규모의 텍스트 데이터로 훈련된 큰 규모의 언어 모델을 의미합니다. 이러한 모델은 다양한 자연어 이해 및 생성 작업에 사용됩니다.\\n예시: OpenAI의 GPT 시리즈는 대표적인 대규모 언어 모델입니다.\\n연관키워드: 자연어 처리, 딥러닝, 텍스트 생성\\n\\nFAISS (Facebook AI Similarity Search)\\n\\n정의: FAISS는 페이스북에서 개발한 고속 유사성 검색 라이브러리로, 특히 대규모 벡터 집합에서 유사 벡터를 효과적으로 검색할 수 있도록 설계되었습니다.\\n예시: 수백만 개의 이미지 벡터 중에서 비슷한 이미지를 빠르게 찾는 데 FAISS가 사용될 수 있습니다.\\n연관키워드: 벡터 검색, 머신러닝, 데이터베이스 최적화\\n\\nOpen Source'),\n",
       " Document(metadata={'source': './data/nlp-keywords.txt'}, page_content='GPT (Generative Pretrained Transformer)\\n\\n정의: GPT는 대규모의 데이터셋으로 사전 훈련된 생성적 언어 모델로, 다양한 텍스트 기반 작업에 활용됩니다. 이는 입력된 텍스트에 기반하여 자연스러운 언어를 생성할 수 있습니다.\\n예시: 사용자가 제공한 질문에 대해 자세한 답변을 생성하는 챗봇은 GPT 모델을 사용할 수 있습니다.\\n연관키워드: 자연어 처리, 텍스트 생성, 딥러닝\\n\\nInstructGPT\\n\\n정의: InstructGPT는 사용자의 지시에 따라 특정한 작업을 수행하기 위해 최적화된 GPT 모델입니다. 이 모델은 보다 정확하고 관련성 높은 결과를 생성하도록 설계되었습니다.\\n예시: 사용자가 \"이메일 초안 작성\"과 같은 특정 지시를 제공하면, InstructGPT는 관련 내용을 기반으로 이메일을 작성합니다.\\n연관키워드: 인공지능, 자연어 이해, 명령 기반 처리\\n\\nKeyword Search')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 유사도 검색\n",
    "db.similarity_search(\"TF IDF 에 대하여 알려줘\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './data/book_document.txt'}, page_content='### 제목 : 숫자는 거짓말을 한다\\n- 저자 : 알베르토 카이로\\n- 발행처 : 웅진지식하우스 20201013\\n- isbn : 9788901245591\\n- 청구기호 : 일 310.1-알848ㅅ\\n- 등록번호 : CLP000184432\\n- 소장위치 : 첫 번째 책장 첫 번째 열\\n\\n\\n### 제목 : 피노자의 형이상학\\n- 저자 : 김은주\\n- 발행처 : 민음사 20240503\\n- isbn : 9788937446009\\n- 청구기호 : 일 166.2-김68ㅅ\\n- 등록번호 : CLP000299472\\n- 소장위치 : 첫 번째 책장 첫 번째 열\\n\\n\\n### 제목 : 파이썬 머신러닝 판다스 데이터 분석 (개정판)\\n- 저자 : 오승환\\n- 발행처 : 정보문화사 20240625\\n- isbn : 9788956749808\\n- 청구기호 : 일 001.12-오98o\\n- 등록번호 : CLP000178599\\n- 소장위치 : 두 번재 책장 첫 번째 열\\n\\n\\n### 제목 : 모두의 파이썬\\n- 저자 : 이승찬\\n- 발행처 : 길벗 2017\\n- isbn : 9791160505856\\n- 청구기호 : 일 005.12-이58ㅁ\\n- 등록번호 : CRX000029216\\n- 소장위치 : 첫 번째 책장 두 번 째 열'),\n",
       " Document(metadata={'source': './data/book_document.txt'}, page_content='### 제목 : 손에잡히는 파이썬\\n- 저자 : 문용준\\n- 발행처 : BJpublic(비제이퍼블릭) 2018\\n- isbn : 9791186697726\\n- 청구기호 : 일 005.133-문66ㅅ\\n- 등록번호 : CLP000178588\\n- 소장위치 : 세 번째 책장 두 번 째 열\\n\\n\\n### 제목 : 작심 3일 파이썬 Python\\n- 저자 : 황덕창\\n- 발행처 : 스포트라잇북 20190425\\n- isbn : 9791187431169\\n- 청구기호 : abc\\n- 등록번호 : CLP000178589\\n- 소장위치 : 두 번째 책장 세 번 째 열\\n\\n\\n### 제목 : 돈의 속성 (300쇄 리커버에디션)\\n- 저자 : 김승호\\n- 발행처 : 스노우폭스북스 20200615\\n- isbn : 9791188331796\\n- 청구기호 : 일 001.12-김58ㅁ\\n- 등록번호 : SBBB000029216\\n- 소장위치 : 두 번째 책장 세 번째 열')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# k 값 지정\n",
    "db.similarity_search(\"파이썬에 대한 책이 있나요?\", k=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
